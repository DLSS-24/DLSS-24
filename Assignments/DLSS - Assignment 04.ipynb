{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b5dcd8",
   "metadata": {},
   "source": [
    "# Deep Learning for Social Sciences\n",
    "## Assignment 04\n",
    "To be handed in by 16.7.2024 23:59 at [https://github.com/orgs/DLSS-24/](https://github.com/orgs/DLSS-24/).\n",
    "\n",
    "These materials were developed by [Max Pellert](https://mpellert.at) building on work by [Simon J.D. Prince](https://udlbook.github.io/udlbook/).\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions. Any references (equations, sections, etc) refer to the book \"Understanding Deep Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349ddc5",
   "metadata": {},
   "source": [
    "### Task 01.01 (5 points)\n",
    "\n",
    "This part builds the self-attention mechanism from scratch, as discussed in section 12.2 of the book \"Understanding Deep Learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c4fe6",
   "metadata": {},
   "source": [
    "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d78dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 3\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "  all_x.append(np.random.normal(size=(D,1)))\n",
    "# Print out the list\n",
    "print(all_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419537d4",
   "metadata": {},
   "source": [
    "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_q = np.random.normal(size=(D,D))\n",
    "omega_k = np.random.normal(size=(D,D))\n",
    "omega_v = np.random.normal(size=(D,D))\n",
    "beta_q = np.random.normal(size=(D,1))\n",
    "beta_k = np.random.normal(size=(D,1))\n",
    "beta_v = np.random.normal(size=(D,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956ff79",
   "metadata": {},
   "source": [
    "Now let's compute the queries, keys, and values for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b256c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make three lists to store queries, keys, and values\n",
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "# For every input\n",
    "for x in all_x:\n",
    "  # TODO -- compute the keys, queries and values.\n",
    "  # Replace these three lines\n",
    "  query = np.ones_like(x)\n",
    "  key = np.ones_like(x)\n",
    "  value = np.ones_like(x)\n",
    "\n",
    "  all_queries.append(query)\n",
    "  all_keys.append(key)\n",
    "  all_values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c705f",
   "metadata": {},
   "source": [
    "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(items_in):\n",
    "\n",
    "  # TODO Compute the elements of items_out\n",
    "  # Replace this line\n",
    "  items_out = items_in.copy()\n",
    "\n",
    "  return items_out ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb51a26",
   "metadata": {},
   "source": [
    "Now compute the self attention values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c66934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list for output\n",
    "all_x_prime = []\n",
    "\n",
    "# For each output\n",
    "for n in range(N):\n",
    "  # Create list for dot products of query N with all keys\n",
    "  all_km_qn = []\n",
    "  # Compute the dot products\n",
    "  for key in all_keys:\n",
    "    # TODO -- compute the appropriate dot product\n",
    "    # Replace this line\n",
    "    dot_product = 1\n",
    "\n",
    "    # Store dot product\n",
    "    all_km_qn.append(dot_product)\n",
    "\n",
    "  # Compute dot product\n",
    "  attention = softmax(all_km_qn)\n",
    "  # Print result (should be positive sum to one)\n",
    "  print(\"Attentions for output \", n)\n",
    "  print(attention)\n",
    "\n",
    "  # TODO: Compute a weighted sum of all of the values according to the attention\n",
    "  # (equation 12.3)\n",
    "  # Replace this line\n",
    "  x_prime = np.zeros((D,1))\n",
    "\n",
    "  all_x_prime.append(x_prime)\n",
    "\n",
    "\n",
    "# Print out true values to check you have it correct\n",
    "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca0270",
   "metadata": {},
   "source": [
    "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
    "\n",
    "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute self attention in matrix form\n",
    "def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "\n",
    "  # TODO -- Write this function\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Apply softmax to calculate attentions\n",
    "  # 4. Weight values by attentions\n",
    "  # Replace this line\n",
    "  X_prime = np.zeros_like(X);\n",
    "\n",
    "\n",
    "  return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854702c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data into matrix\n",
    "X = np.zeros((D, N))\n",
    "X[:,0] = np.squeeze(all_x[0])\n",
    "X[:,1] = np.squeeze(all_x[1])\n",
    "X[:,2] = np.squeeze(all_x[2])\n",
    "\n",
    "# Run the self attention mechanism\n",
    "X_prime = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f7945",
   "metadata": {},
   "source": [
    "If you did this correctly, the values should be the same as before.\n",
    "\n",
    "TODO:  \n",
    "\n",
    "Print out the attention matrix<br>You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62968c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute self attention in matrix form\n",
    "def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "\n",
    "  # TODO -- Write this function\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Compute dot products\n",
    "  # 3. Scale the dot products as in equation 12.9\n",
    "  # 4. Apply softmax to calculate attentions\n",
    "  # 5. Weight values by attentions\n",
    "  # Replace this line\n",
    "  X_prime = np.zeros_like(X);\n",
    "\n",
    "  return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337cd583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17a518",
   "metadata": {},
   "source": [
    "TODO -- Investigate whether the self-attention mechanism is covariant with respect to permutation.\n",
    "If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe9038",
   "metadata": {},
   "source": [
    "### Task 01.02 (5 points)\n",
    "\n",
    "This part builds a **multihead** self-attention mechanism as in figure 12.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040bd846",
   "metadata": {},
   "source": [
    "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 6\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "# Create an empty list\n",
    "X = np.random.normal(size=(D,N))\n",
    "# Print X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06810b7e",
   "metadata": {},
   "source": [
    "We'll need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4). We'll use two heads, and (as in the figure), we'll make the queries keys and values of size D/H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of heads\n",
    "H = 2\n",
    "# QDV dimension\n",
    "H_D = int(D/H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "omega_q1 = np.random.normal(size=(H_D,D))\n",
    "omega_k1 = np.random.normal(size=(H_D,D))\n",
    "omega_v1 = np.random.normal(size=(H_D,D))\n",
    "beta_q1 = np.random.normal(size=(H_D,1))\n",
    "beta_k1 = np.random.normal(size=(H_D,1))\n",
    "beta_v1 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "omega_q2 = np.random.normal(size=(H_D,D))\n",
    "omega_k2 = np.random.normal(size=(H_D,D))\n",
    "omega_v2 = np.random.normal(size=(H_D,D))\n",
    "beta_q2 = np.random.normal(size=(H_D,1))\n",
    "beta_k2 = np.random.normal(size=(H_D,1))\n",
    "beta_v2 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_c = np.random.normal(size=(D,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb347764",
   "metadata": {},
   "source": [
    "Now let's compute the multiscale self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # Compute softmax (numpy broadcasts denominator to all rows automatically)\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1454d62",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
    "\n",
    "  # TODO Write the multihead scaled self-attention mechanism.\n",
    "  # Replace this line\n",
    "  X_prime = np.zeros_like(X) ;\n",
    "\n",
    "\n",
    "  return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c)\n",
    "\n",
    "# Print out the results\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"True values:\")\n",
    "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
    "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
    "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
    "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
    "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
    "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
    "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
    "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")\n",
    "\n",
    "# If your answers don't match, then make sure that you are doing the scaling, and make sure the scaling value is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4559e",
   "metadata": {},
   "source": [
    "### Task 01.03 (5 points)\n",
    "\n",
    "This part builds set of tokens from a text string as in figure 12.8 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a sailor went to sea sea sea \"+\\\n",
    "                  \"to see what he could see see see \"+\\\n",
    "                  \"but all that he could see see see \"+\\\n",
    "                  \"was the bottom of the deep blue sea sea sea\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a715555",
   "metadata": {},
   "source": [
    "Tokenize the input sentence\n",
    "\n",
    "At its simplest, tokens are sequences of individual characters seperated by the </w> whitespace token. So, let's use this to take the text apart.\n",
    "\n",
    "The tokenized text is stored in a structure that represents each word as tokens together with the count of how often that word occurs.  We'll call this the vocabulary with counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vocabulary(text):\n",
    "  vocab = collections.defaultdict(int)\n",
    "  words = text.strip().split()\n",
    "  for word in words:\n",
    "      vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = initialize_vocabulary(text)\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdc9bf",
   "metadata": {},
   "source": [
    "Find all the characters in the current vocabulary and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a64cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_frequencies(vocab):\n",
    "  tokens = collections.defaultdict(int)\n",
    "  for word, freq in vocab.items():\n",
    "      word_tokens = word.split()\n",
    "      for token in word_tokens:\n",
    "          tokens[token] += freq\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokens_and_frequencies(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14684e4",
   "metadata": {},
   "source": [
    "Find each pair of adjacent tokens in the vocabulary and count them. We will subsequently merge the most frequently occurring pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_and_counts(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908018b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_pairs_and_counts(vocab)\n",
    "print('Pairs: {}'.format(pairs))\n",
    "print('Number of distinct pairs: {}'.format(len(pairs)))\n",
    "\n",
    "most_frequent_pair = max(pairs, key=pairs.get)\n",
    "print('Most frequent pair: {}'.format(most_frequent_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5113f",
   "metadata": {},
   "source": [
    "Merge the instances of the most frequent pair in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655eb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair_in_vocabulary(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in vocab_in:\n",
    "        word_out = p.sub(''.join(pair), word)\n",
    "        vocab_out[word_out] = vocab_in[word]\n",
    "    return vocab_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675de3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = merge_pair_in_vocabulary(most_frequent_pair, vocab)\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d741",
   "metadata": {},
   "source": [
    "Update the collection of tokens, which now include the 'se' as one combined token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25025e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = get_tokens_and_frequencies(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ddb606",
   "metadata": {},
   "source": [
    "Now let's write the full tokenization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- write this routine by filling in this missing parts,\n",
    "# calling the above routines\n",
    "def tokenize(text, num_merges):\n",
    "  # Initialize the vocabulary from the input text\n",
    "  # vocab = (your code here)\n",
    "\n",
    "  for i in range(num_merges):\n",
    "    # Find the tokens and how often they occur in the vocabulary\n",
    "    # tokens = (your code here)\n",
    "\n",
    "    # Find the pairs of adjacent tokens and their counts\n",
    "    # pairs = (your code here)\n",
    "\n",
    "    # Find the most frequent pair\n",
    "    # most_frequent_pair = (your code here)\n",
    "    print('Most frequent pair: {}'.format(most_frequent_pair))\n",
    "\n",
    "    # Merge the code in the vocabulary\n",
    "    # vocab = (your code here)\n",
    "\n",
    "  # Find the tokens and how often they occur in the vocabulary one last time\n",
    "  # tokens = (your code here)\n",
    "\n",
    "  return tokens, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08055f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab = tokenize(text, num_merges=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('Vocabulary: {}'.format(vocab))\n",
    "print('Size of vocabulary: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd0621",
   "metadata": {},
   "source": [
    "TODO - Consider the input text:\n",
    "\n",
    "\"How much wood could a woodchuck chuck if a woodchuck could chuck wood\"\n",
    "\n",
    "How many tokens will there be initially and what will they be?<br>\n",
    "How many tokens will there be if we run the tokenization routine for the maximum number of iterations (merges)?\n",
    "\n",
    "When you've made your predictions, run the code and see if you are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb341038",
   "metadata": {},
   "source": [
    "### Task 01.04 (5 points)\n",
    "\n",
    "This practical investigates neural decoding from transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0afff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0625fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d911e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19837353",
   "metadata": {},
   "source": [
    "# Decoding from GPT-2\n",
    "\n",
    "This tutorial investigates how to use GPT-2 (the forerunner of GPT-3, GPT-4, etc) to generate text. There are a number of ways to do this that trade-off the realism of the text against the amount of variation.\n",
    "\n",
    "At every stage, GPT-2 takes an input string and returns a probability for each of the possible subsequent tokens.  We can choose what to do with these probability.  We could always *greedily choose* the most likely next token, or we could draw a *sample* randomly according to the probabilities.  There are also intermediate strategies such as *top-k sampling* and *nucleus sampling*, that have some controlled randomness.\n",
    "\n",
    "We'll also investigate *beam search* -- the idea is that rather than greedily take the next best token at each stage, we maintain a set of hypotheses  (beams)as we add each subsequent token and return the most likely overall hypothesis.  This is not necessarily the same result we get from greedily choosing the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cccdd2",
   "metadata": {},
   "source": [
    "First, let's investigate the token themselves. The code below prints out the vocabulary size and shows 20 random tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef235de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "print(\"Number of tokens in dictionary = %d\"%(tokenizer.vocab_size))\n",
    "for i in range(20):\n",
    "  index = np.random.randint(tokenizer.vocab_size)\n",
    "  print(\"Token: %d \"%(index)+tokenizer.decode(torch.tensor(index), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238cd08",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "Each time we run GPT-2, it will take in a set of tokens and return a probability over each of the possible next tokens. The simplest thing we could do is to just draw a sample from this probability distribution each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(input_tokens, model, tokenizer):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "  # TODO Draw a random token according to the probabilities\n",
    "  # next_token should be an array with an sole integer in it (as below)\n",
    "  # Use:  https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "  # Replace this line\n",
    "  next_token = [5000]\n",
    "\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output:\n",
    "# \"The best thing about University of Konstanz is that they don't need any travel expenses\"\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(8):\n",
    "    input_tokens = sample_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Modify the code below by changing the number of tokens generated and the initial sentence\n",
    "# to get a feel for how well this works. Since we didn't reset the seed, it will give a different\n",
    "# answer every time that you run it.\n",
    "\n",
    "# TODO Experiment with changing this line:\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "# TODO Experiment with changing this line:\n",
    "for i in range(8):\n",
    "    input_tokens = sample_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837831b4",
   "metadata": {},
   "source": [
    "# Greedy token selection\n",
    "\n",
    "You probably (correctly) got the impression that the text from pure sampling of the probability model can be kind of random.  How about if we choose most likely token at each step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa56c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_next_token(input_tokens, model, tokenizer):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # TODO -- find the token index with the maximum probability\n",
    "  # It should be returns as a list (i.e., put squared brackets around it)\n",
    "  # Use https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
    "  # Replace this line\n",
    "  next_token = [5000]\n",
    "\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output:\n",
    "# The best thing about University of Konstanz is that it's a place where you can learn\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(9):\n",
    "    input_tokens = get_best_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc899433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Modify the code below by changing the number of tokens generated and the initial sentence\n",
    "# to get a feel for how well this works.\n",
    "\n",
    "# TODO Experiment with changing this line:\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "# TODO Experiment with changing this line:\n",
    "for i in range(9):\n",
    "    input_tokens = get_best_next_token(input_tokens, model, tokenizer)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad68a7",
   "metadata": {},
   "source": [
    "# Top-K sampling\n",
    "\n",
    "You probably noticed that the greedy strategy produces quite realistic text, but it's kind of boring.  It produces generic answers.  Also, if this was a chatbot, then we wouldn't necessarily want it to produce the same answer to a question each time.  \n",
    "\n",
    "Top-K sampling is a compromise strategy that samples randomly from the top K most probable tokens.  We could just choose them with a uniform distribution, or (as here) we could sample them according to their original probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_token(input_tokens, model, tokenizer, k=20):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Draw a sample from the top K most likely tokens.\n",
    "  # Take copy of the probabilities and sort from largest to smallest (use np.sort)\n",
    "  # TODO -- replace this line\n",
    "  sorted_prob_over_tokens =  prob_over_tokens\n",
    "\n",
    "  # Find the probability at the k'th position\n",
    "  # TODO -- replace this line\n",
    "  kth_prob_value = 0.0\n",
    "\n",
    "  # Set all probabilities below this value to zero\n",
    "  prob_over_tokens[prob_over_tokens<kth_prob_value] = 0\n",
    "\n",
    "  # Renormalize the probabilities so that they sum to one\n",
    "  # TODO -- replace this line\n",
    "  prob_over_tokens = prob_over_tokens\n",
    "\n",
    "\n",
    "  # Draw random token\n",
    "  next_token = np.random.choice(len(prob_over_tokens), 1, replace=False, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae335eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output:\n",
    "# The best thing about University of Konstanz is that we can use it for any purpose\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(8):\n",
    "    input_tokens = get_top_k_token(input_tokens, model, tokenizer, k=10)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fafd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Experiment with different values of k\n",
    "# If you set it to a lower number (say 3) the text will be less random\n",
    "# If you set it to a higher number (say 5000) the text will be more random\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(8):\n",
    "    input_tokens = get_top_k_token(input_tokens, model, tokenizer, k=10)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23eca6c",
   "metadata": {},
   "source": [
    "# Nucleus sampling\n",
    "\n",
    "Top-K sampling has the disadvantage that sometimes there are only a few plausible next tokens, and sometimes there are a lot. How do we adapt to this situation?  One way is to sample from a fixed proportion of the probability mass. That is we order the tokens in terms of probability and cut off the possibility of sampling when the cumulative sum is greater than a threshold.\n",
    "\n",
    "This way, we adapt the number of possible tokens that we can choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh=0.25):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Find the most likely tokens that make up the first (thresh) of the probability\n",
    "  # TODO -- sort the probabilities in decreasing order\n",
    "  # Replace this line\n",
    "  sorted_probs_decreasing = prob_over_tokens\n",
    "  # TODO -- compute the cumulative sum of these probabilities\n",
    "  # Replace this line\n",
    "  cum_sum_probs = sorted_probs_decreasing\n",
    "\n",
    "\n",
    "\n",
    "  # Find index where that the cumulative sum is greater than the threshold\n",
    "  thresh_index = np.argmax(cum_sum_probs>thresh)\n",
    "  print(\"Choosing from %d tokens\"%(thresh_index))\n",
    "  # TODO:  Find the probability value to threshold\n",
    "  # Replace this line:\n",
    "  thresh_prob = 0.5\n",
    "\n",
    "\n",
    "\n",
    "  # Set any probabilities less than this to zero\n",
    "  prob_over_tokens[prob_over_tokens<thresh_prob] = 0\n",
    "  # Renormalize\n",
    "  prob_over_tokens = prob_over_tokens / np.sum(prob_over_tokens)\n",
    "  # Draw random token\n",
    "  next_token = np.random.choice(len(prob_over_tokens), 1, replace=False, p=prob_over_tokens)\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output:\n",
    "# The best thing about University of Konstanz is that it's a great place to learn\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(8):\n",
    "    input_tokens = get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh = 0.2)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- experiment with setting the threshold probability to larger or smaller values\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "for i in range(8):\n",
    "    input_tokens = get_nucleus_sampling_token(input_tokens, model, tokenizer, thresh = 0.2)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e7918",
   "metadata": {},
   "source": [
    "# Beam search\n",
    "\n",
    "All of the methods we've seen so far choose the tokens one by one.  But this isn't necessarily sensible.  Even greedily choosing the best token doesn't necessarily retrieve the sequence with the highest probability.  It might be that the most likely token only has very unlikely tokens following it.\n",
    "\n",
    "Beam search maintains $K$ hypotheses about the best possible continuation.  It starts with the top $K$ continuations.  Then for each of those, it finds the top K continuations, giving $K^2$ hypotheses.  Then it retains just the top $K$ of these so that the number of hypotheses stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This routine returns the k'th most likely next token.\n",
    "# If k =0 then it returns the most likely token, if k=1 it returns the next most likely and so on\n",
    "# We will need this for beam search\n",
    "def get_kth_most_likely_token(input_tokens, model, tokenizer, k):\n",
    "  # Run model to get prediction over next output\n",
    "  outputs = model(input_ids = input_tokens['input_ids'], attention_mask = input_tokens['attention_mask'])\n",
    "  # Find prediction\n",
    "  prob_over_tokens = F.softmax(outputs.logits, dim=-1).detach().numpy()[0,-1]\n",
    "\n",
    "  # Find the k'th most likely token\n",
    "  # TODO Sort the probabilities from largest to smallest\n",
    "  # Replace this line:\n",
    "  sorted_prob_over_tokens = prob_over_tokens\n",
    "  # TODO Find the k'th sorted probability\n",
    "  # Replace this line\n",
    "  kth_prob_value = prob_over_tokens[0]\n",
    "\n",
    "\n",
    "\n",
    "  # Find position of this token.\n",
    "  next_token = np.where(prob_over_tokens == kth_prob_value)[0]\n",
    "\n",
    "  # Append token to sentence\n",
    "  output_tokens = input_tokens\n",
    "  output_tokens[\"input_ids\"] = torch.cat((output_tokens['input_ids'],torch.tensor([next_token])),dim=1)\n",
    "  output_tokens['attention_mask'] = torch.cat((output_tokens['attention_mask'],torch.tensor([[1]])),dim=1)\n",
    "  output_tokens['last_token_prob'] = prob_over_tokens[next_token]\n",
    "  output_tokens['log_prob'] = output_tokens['log_prob'] + np.log(prob_over_tokens[next_token])\n",
    "  return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can test this code and see that if we choose the 2nd most likely (K=1) token each time\n",
    "# then we get much better generation results than if we choose the 2001st most likely token\n",
    "\n",
    "# Expected output:\n",
    "# The best thing about University of Konstanz is the opportunity for people who want a career to get\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "input_tokens['log_prob'] = 0.0\n",
    "for i in range(10):\n",
    "    input_tokens = get_kth_most_likely_token(input_tokens, model, tokenizer, k=1)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n",
    "# Expected output:\n",
    "# The best thing about University of Konstanz is laying qualifications personal address detailed getting stressed previously threw vast\n",
    "input_txt = \"The best thing about University of Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "input_tokens['log_prob'] = 0.0\n",
    "for i in range(10):\n",
    "    input_tokens = get_kth_most_likely_token(input_tokens, model, tokenizer, k=2000)\n",
    "    print(tokenizer.decode(input_tokens[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n",
    "# TODO -- play around with different values of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bdcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out each beam plus the log probability\n",
    "def print_beams(beams):\n",
    "  for index,beam in enumerate(beams):\n",
    "    print(\"Beam %d, Prob %3.3f: \"%(index,beam['log_prob'])+tokenizer.decode(beam[\"input_ids\"][0], skip_special_tokens=True))\n",
    "  print('---')\n",
    "\n",
    "\n",
    "# TODO:  Read this code carefully!\n",
    "def do_beam_search(input_tokens_in, model, tokenizer, n_beam=5, beam_length=10):\n",
    "  # Store beams in a list\n",
    "  input_tokens['log_prob'] = 0.0\n",
    "\n",
    "  # Initialize with n_beam most likely continuations\n",
    "  beams = [None] * n_beam\n",
    "  for c_k in range(n_beam):\n",
    "    beams[c_k] = dict(input_tokens_in)\n",
    "    beams[c_k] = get_kth_most_likely_token(beams[c_k], model, tokenizer, c_k)\n",
    "\n",
    "  print_beams(beams)\n",
    "\n",
    "  # For each token in the sequence we will add\n",
    "  for c_pos in range(beam_length-1):\n",
    "    # Now for each beam, we continue it in the most likely ways, making n_beam*n_beam type hypotheses\n",
    "    beams_all = [None] * (n_beam*n_beam)\n",
    "    log_probs_all = np.zeros(n_beam*n_beam)\n",
    "    # For each current hypothesis\n",
    "    for c_beam in range(n_beam):\n",
    "      # For each continuation\n",
    "      for c_k in range(n_beam):\n",
    "        # Store the continuation and the probability\n",
    "        beams_all[c_beam * n_beam + c_k] = dict(get_kth_most_likely_token(beams[c_beam], model, tokenizer, c_k))\n",
    "        log_probs_all[c_beam * n_beam + c_k] = beams_all[c_beam * n_beam + c_k]['log_prob']\n",
    "\n",
    "    # Keep the best n_beams sequences with the highest probabilities\n",
    "    sorted_index = np.argsort(np.array(log_probs_all)*-1)\n",
    "    for c_k in range(n_beam):\n",
    "      beams[c_k] = dict(beams_all[sorted_index[c_k]])\n",
    "\n",
    "    # Print the beams\n",
    "    print_beams(beams)\n",
    "\n",
    "  return beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80854517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output:\n",
    "# The best thing about University of Konstanz is that it's a place where you don't have to\n",
    "\n",
    "set_seed(0)\n",
    "input_txt = \"The best thing about Uni Konstanz is\"\n",
    "input_tokens = tokenizer(input_txt, return_tensors='pt')\n",
    "\n",
    "# Now let's call the beam search\n",
    "# It takes a while as it has to run the model multiple times to add a token\n",
    "n_beams = 5\n",
    "best_beam = do_beam_search(input_tokens,model,tokenizer)\n",
    "print(\"Beam search result:\")\n",
    "print(tokenizer.decode(best_beam[\"input_ids\"][0], skip_special_tokens=True))\n",
    "\n",
    "# You should see that the best answer is not the same as the greedy solution we found above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
